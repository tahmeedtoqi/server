{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ebf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# number of workers in .map() call\n",
    "num_proc = 8\n",
    "\n",
    "# initialize the tokenizer (GPT-2 encoding)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# path to your data folder containing .txt files\n",
    "data_folder = 'data'\n",
    "\n",
    "def process_files(data_folder):\n",
    "    files = [f for f in os.listdir(data_folder) if f.endswith('.txt')]\n",
    "    all_ids = []\n",
    "    file_lengths = {}\n",
    "\n",
    "    print(f\"Found {len(files)} .txt files.\")\n",
    "    for file in files:\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        print(f\"Processing {file}...\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            ids = enc.encode_ordinary(text)\n",
    "            ids.append(enc.eot_token)\n",
    "            all_ids.extend(ids)\n",
    "            file_lengths[file] = len(ids)\n",
    "\n",
    "    return all_ids, file_lengths, files\n",
    "\n",
    "def restore_text_from_bin(bin_filename, file_lengths):\n",
    "    dtype = np.uint16\n",
    "    arr = np.memmap(bin_filename, dtype=dtype, mode='r')\n",
    "    restored_files = {}\n",
    "    idx = 0\n",
    "    for file, length in file_lengths.items():\n",
    "        restored_ids = arr[idx:idx + length]\n",
    "        restored_text = enc.decode(restored_ids[:-1])  # remove EOT\n",
    "        restored_files[file] = restored_text\n",
    "        idx += length\n",
    "    return restored_files\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Process files\n",
    "    all_token_ids, file_lengths, files = process_files(data_folder)\n",
    "\n",
    "    output_dir = 'output/bin'\n",
    "    output_filename = os.path.join(output_dir, 'dataset.bin')\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Safe deletion of existing binary file\n",
    "    if os.path.exists(output_filename):\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            os.remove(output_filename)\n",
    "        except PermissionError:\n",
    "            print(f\"Could not delete {output_filename}. It may still be in use.\")\n",
    "\n",
    "    dtype = np.uint16\n",
    "    arr_len = len(all_token_ids)\n",
    "\n",
    "    arr = np.memmap(output_filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "    for idx in tqdm(range(0, arr_len, num_proc), desc=f'writing {output_filename}'):\n",
    "        batch = all_token_ids[idx:idx + num_proc]\n",
    "        arr[idx:idx + len(batch)] = batch\n",
    "\n",
    "    arr.flush()\n",
    "    print(f\"Dataset has been saved to {output_filename}\")\n",
    "\n",
    "    # Restore from binary\n",
    "    restored_files = restore_text_from_bin(output_filename, file_lengths)\n",
    "\n",
    "    # Save restored files to desired folder\n",
    "    restored_output_dir = r'C:\\Users\\tahme\\Desktop\\server\\output\\restored_txt'\n",
    "    os.makedirs(restored_output_dir, exist_ok=True)\n",
    "\n",
    "    for file, content in restored_files.items():\n",
    "        restored_file_path = os.path.join(restored_output_dir, file)\n",
    "        with open(restored_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"Saved restored: {restored_file_path}\")\n",
    "\n",
    "    # Size comparison\n",
    "    original_sizes = [os.path.getsize(os.path.join(data_folder, file)) / (1024 * 1024) for file in files]\n",
    "    total_original_size = sum(original_sizes)\n",
    "    bin_size = os.path.getsize(output_filename) / (1024 * 1024)\n",
    "    restored_sizes = [len(restored_files[file]) * 2 / (1024 * 1024) for file in files]\n",
    "\n",
    "    # Calculate size difference and percentage\n",
    "    size_difference = total_original_size - bin_size\n",
    "    percent_difference = (size_difference / total_original_size) * 100 if total_original_size > 0 else 0\n",
    "\n",
    "    print(\"\\nüìä Size Summary:\")\n",
    "    print(f\"Total Original Size: {total_original_size:.2f} MB\")\n",
    "    print(f\"Binary (.bin) Size:  {bin_size:.2f} MB\")\n",
    "    print(f\"Difference:          {size_difference:.2f} MB\")\n",
    "    print(f\"Compression:         {percent_difference:.2f}%\")\n",
    "\n",
    "    # Plotting histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(original_sizes, bins=5, alpha=0.6, label='Original Text')\n",
    "    plt.hist([bin_size] * len(files), bins=5, alpha=0.6, label='Binary Size')\n",
    "    plt.hist(restored_sizes, bins=5, alpha=0.6, label='Restored Text')\n",
    "    plt.xlabel('Size (MB)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Comparison of File Sizes (Original, Binary, Restored)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Verify correctness\n",
    "    for file in files:\n",
    "        original_text = open(os.path.join(data_folder, file), 'r', encoding='utf-8').read()\n",
    "        restored_text = restored_files[file]\n",
    "        if original_text == restored_text:\n",
    "            print(f\"‚úÖ Restoration of {file} was successful!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Restoration of {file} failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40dc8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Directories\n",
    "data_dir = \"data\"\n",
    "output_webp_dir = os.path.join(\"output\", \"webp\")\n",
    "output_restored_dir = os.path.join(\"output\", \"restored\")\n",
    "os.makedirs(output_webp_dir, exist_ok=True)\n",
    "os.makedirs(output_restored_dir, exist_ok=True)\n",
    "\n",
    "total_original_size = 0\n",
    "total_webp_size = 0\n",
    "first_original_path = None\n",
    "first_webp_path = None\n",
    "first_restored_path = None\n",
    "\n",
    "# Convert images to WebP (lossy mode)\n",
    "for idx, filename in enumerate(os.listdir(data_dir)):\n",
    "    if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        original_size = os.path.getsize(file_path)\n",
    "        total_original_size += original_size\n",
    "\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        \n",
    "        # Save as WebP (lossy)\n",
    "        webp_filename = os.path.splitext(filename)[0] + \".webp\"\n",
    "        webp_path = os.path.join(output_webp_dir, webp_filename)\n",
    "        image.save(webp_path, format=\"WEBP\", quality=80)  # Lossy WebP at quality 80\n",
    "        webp_size = os.path.getsize(webp_path)\n",
    "        total_webp_size += webp_size\n",
    "        \n",
    "        # Restore from WebP back to JPG\n",
    "        restored_filename = os.path.splitext(filename)[0] + \"_restored.jpg\"\n",
    "        restored_path = os.path.join(output_restored_dir, restored_filename)\n",
    "        restored_image = Image.open(webp_path).convert(\"RGB\")\n",
    "        restored_image.save(restored_path, format=\"JPEG\", quality=95)  # Restore as high-quality JPG\n",
    "\n",
    "        # Store first image paths for comparison\n",
    "        if first_original_path is None:\n",
    "            first_original_path = file_path\n",
    "            first_webp_path = webp_path\n",
    "            first_restored_path = restored_path\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total Original Size: {total_original_size / 1024:.2f} KB\")\n",
    "print(f\"Total WebP Size: {total_webp_size / 1024:.2f} KB\")\n",
    "if total_original_size > 0:\n",
    "    size_ratio = (total_webp_size / total_original_size) * 100\n",
    "    reduction = 100 - size_ratio\n",
    "    print(f\"Size Ratio (WebP/Original): {size_ratio:.2f}%\")\n",
    "    print(f\"Size Reduction: {reduction:.2f}%\")\n",
    "else:\n",
    "    print(\"No images found.\")\n",
    "\n",
    "# --- Data Visualization ---\n",
    "if first_original_path and first_webp_path and first_restored_path:\n",
    "    original_img = np.array(Image.open(first_original_path).convert(\"RGB\"))\n",
    "    webp_img = np.array(Image.open(first_webp_path).convert(\"RGB\"))\n",
    "    restored_img = np.array(Image.open(first_restored_path).convert(\"RGB\"))\n",
    "\n",
    "    # Flatten the RGB values\n",
    "    original_pixels = original_img.flatten()\n",
    "    webp_pixels = webp_img.flatten()\n",
    "    restored_pixels = restored_img.flatten()\n",
    "\n",
    "    plt.figure(figsize=(18, 5))\n",
    "\n",
    "    # Histogram of Original Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(original_pixels, bins=256, color='blue', alpha=0.7, label=\"Original\")\n",
    "    plt.xlabel(\"Pixel Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Pixel Distribution - Original Image\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Histogram of WebP Image\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(webp_pixels, bins=256, color='red', alpha=0.7, label=\"WebP (Lossy)\")\n",
    "    plt.xlabel(\"Pixel Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Pixel Distribution - WebP Image\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Histogram of Restored JPG Image\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(restored_pixels, bins=256, color='green', alpha=0.7, label=\"Restored JPG\")\n",
    "    plt.xlabel(\"Pixel Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Pixel Distribution - Restored JPG Image\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd \n",
    "import msgpack\n",
    "import os\n",
    "import subprocess\n",
    "import fitz\n",
    "\n",
    "# ======================== Configuration ========================\n",
    "INPUT_PDF = r'C:\\Users\\tahme\\Desktop\\server\\data\\M.M. THAMEED THOKY, D.pdf'\n",
    "COMPRESSED_BIN = r'C:\\Users\\tahme\\Desktop\\server\\output\\bin\\output.bin'\n",
    "RESTORED_PDF = r'C:\\Users\\tahme\\Desktop\\server\\output\\restored_txt\\restored.pdf'\n",
    "COMPRESSION_LEVEL = 22\n",
    "GHOSTSCRIPT_PATH = r'C:\\Program Files\\gs\\gs10.05.0\\bin\\gswin64c.exe'\n",
    "# ================================================================\n",
    "\n",
    "GHOSTSCRIPT_OPTIONS = [\n",
    "    \"-dPDFSETTINGS=/ebook\",\n",
    "    \"-dColorImageResolution=72\",\n",
    "    \"-dGrayImageResolution=72\",\n",
    "    \"-dMonoImageResolution=72\",\n",
    "    \"-dDownsampleColorImages=true\",\n",
    "    \"-dDownsampleGrayImages=true\",\n",
    "    \"-dDownsampleMonoImages=true\",\n",
    "    \"-dAutoFilterColorImages=false\",\n",
    "    \"-dAutoFilterGrayImages=false\",\n",
    "    \"-dColorImageDownsampleType=/Bicubic\",\n",
    "    \"-dGrayImageDownsampleType=/Bicubic\",\n",
    "    \"-dMonoImageDownsampleType=/Bicubic\"\n",
    "]\n",
    "\n",
    "def format_size(size_bytes: int) -> str:\n",
    "    \"\"\"Convert bytes to human-readable format.\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size_bytes < 1024:\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "    return f\"{size_bytes:.2f} TB\"\n",
    "\n",
    "def optimize_with_ghostscript(input_path: str, output_path: str) -> None:\n",
    "    \"\"\"Fixed path handling for Windows\"\"\"\n",
    "    try:\n",
    "        # Create absolute paths\n",
    "        input_path = os.path.abspath(input_path)\n",
    "        output_path = os.path.abspath(output_path)\n",
    "        \n",
    "        # Verify GS executable exists\n",
    "        if not os.path.exists(GHOSTSCRIPT_PATH):\n",
    "            raise FileNotFoundError(f\"Ghostscript not found at {GHOSTSCRIPT_PATH}\")\n",
    "\n",
    "        # Create output directory\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # Build command with proper Windows quoting\n",
    "        cmd = [\n",
    "            f'\"{GHOSTSCRIPT_PATH}\"',\n",
    "            '-q',\n",
    "            '-dNOPAUSE',\n",
    "            '-dBATCH',\n",
    "            '-sDEVICE=pdfwrite',\n",
    "            f'-sOutputFile=\"{output_path}\"',\n",
    "            *GHOSTSCRIPT_OPTIONS,\n",
    "            f'\"{input_path}\"'\n",
    "        ]\n",
    "\n",
    "        print(\"\\nüîß Running Ghostscript command:\")\n",
    "        print(\" \".join(cmd))\n",
    "\n",
    "        # Execute with proper shell handling\n",
    "        subprocess.run(\" \".join(cmd), check=True, shell=True)\n",
    "\n",
    "        # Verify output\n",
    "        if not os.path.exists(output_path):\n",
    "            raise RuntimeError(\"Optimized PDF not created\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n‚ùå Ghostscript failed with error {e.returncode}\")\n",
    "        raise RuntimeError(\"PDF optimization failed\") from e\n",
    "\n",
    "\n",
    "def pdf_to_bin(pdf_path: str, bin_path: str) -> float:\n",
    "    \"\"\"Two-stage compression pipeline with error handling\"\"\"\n",
    "    try:\n",
    "        print(\"üîß Stage 1: PDF optimization with Ghostscript\")\n",
    "        temp_pdf = \"temp_optimized.pdf\"\n",
    "        \n",
    "        # Create output directory for compressed binary\n",
    "        os.makedirs(os.path.dirname(bin_path), exist_ok=True)\n",
    "        \n",
    "        optimize_with_ghostscript(pdf_path, temp_pdf)\n",
    "\n",
    "        print(\"\\nüîß Stage 2: Zstandard compression\")\n",
    "        if not os.path.exists(temp_pdf):\n",
    "            raise FileNotFoundError(\"Optimized PDF not created\")\n",
    "\n",
    "        with open(temp_pdf, \"rb\") as f:\n",
    "            optimized_bytes = f.read()\n",
    "\n",
    "        # Perform compression\n",
    "        cctx = zstd.ZstdCompressor(level=COMPRESSION_LEVEL)\n",
    "        compressed = cctx.compress(optimized_bytes)\n",
    "\n",
    "        # Package data\n",
    "        data = {\n",
    "            \"zstd_compressed\": compressed,\n",
    "            \"original_size\": os.path.getsize(pdf_path),\n",
    "            \"optimized_size\": len(optimized_bytes)\n",
    "        }\n",
    "\n",
    "        # Write compressed data\n",
    "        with open(bin_path, \"wb\") as f:\n",
    "            f.write(msgpack.packb(data))\n",
    "\n",
    "        final_size = os.path.getsize(bin_path)\n",
    "        return final_size\n",
    "\n",
    "    finally:\n",
    "        # Cleanup temporary file\n",
    "        if os.path.exists(temp_pdf):\n",
    "            os.remove(temp_pdf)\n",
    "\n",
    "def bin_to_pdf(bin_path: str, output_path: str) -> None:\n",
    "    \"\"\"Enhanced decompression with validation\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(bin_path):\n",
    "            raise FileNotFoundError(f\"Compressed file not found: {bin_path}\")\n",
    "\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        with open(bin_path, \"rb\") as f:\n",
    "            data = msgpack.unpackb(f.read())\n",
    "\n",
    "        # Validate sizes\n",
    "        if 'zstd_compressed' not in data:\n",
    "            raise ValueError(\"Invalid compressed data format\")\n",
    "\n",
    "        # Decompress\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        restored = dctx.decompress(data[\"zstd_compressed\"])\n",
    "\n",
    "        # Size validation\n",
    "        if len(restored) != data[\"optimized_size\"]:\n",
    "            raise ValueError(\"Decompressed size mismatch\")\n",
    "\n",
    "        # Write restored PDF\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(restored)\n",
    "\n",
    "    except Exception as e:\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "        raise\n",
    "\n",
    "def print_stats(original_path: str, bin_path: str, optimized_size: int):\n",
    "    \"\"\"Enhanced size reporting\"\"\"\n",
    "    try:\n",
    "        original_size = os.path.getsize(original_path)\n",
    "        compressed_size = os.path.getsize(bin_path)\n",
    "        \n",
    "        print(\"\\nüìä Final Compression Stats:\")\n",
    "        print(f\"‚Ä¢ Original PDF Size:   {format_size(original_size)}\")\n",
    "        print(f\"‚Ä¢ Ghostscript Output:  {format_size(optimized_size)}\")\n",
    "        print(f\"‚Ä¢ Final .bin Size:     {format_size(compressed_size)}\")\n",
    "        \n",
    "        total_reduction = original_size - compressed_size\n",
    "        print(f\"\\nüî• Total Reduction: {format_size(total_reduction)} ({total_reduction/original_size*100:.1f}%)\")\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n‚ùå Error calculating stats: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "    # Verify input file exists\n",
    "        if not os.path.exists(INPUT_PDF):\n",
    "            raise FileNotFoundError(f\"Input PDF not found: {INPUT_PDF}\")\n",
    "\n",
    "        # Create output directories\n",
    "        os.makedirs(os.path.dirname(COMPRESSED_BIN), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(RESTORED_PDF), exist_ok=True)\n",
    "\n",
    "        # Compression workflow\n",
    "        final_size = pdf_to_bin(INPUT_PDF, COMPRESSED_BIN)\n",
    "        print_stats(INPUT_PDF, COMPRESSED_BIN, final_size)\n",
    "        \n",
    "        # Decompression workflow\n",
    "        bin_to_pdf(COMPRESSED_BIN, RESTORED_PDF)\n",
    "        \n",
    "        # Validation\n",
    "        with fitz.open(RESTORED_PDF) as doc:\n",
    "            print(f\"\\n‚úÖ Restoration successful! Pages: {len(doc)}\")\n",
    "            print(\"Metadata:\", doc.metadata)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Critical error: {str(e)}\")\n",
    "        # Cleanup partial files\n",
    "        for path in [COMPRESSED_BIN, RESTORED_PDF]:\n",
    "            if os.path.exists(path):\n",
    "                os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432f3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
